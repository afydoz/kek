19:53:40	 От  Sergey Lyakhov : звук есть
19:54:50	 От  Дмитрий Лабазкин : а как проверить тут?
19:54:56	 От  Александр Соловьев : как бы не нажать тут что то лишнее)
19:55:28	 От  Пользователь Отус : вроде сегодня все ок
19:55:35	 От  Пользователь Отус : печеньку доем и приступим)
19:55:45	 От  Хорев Тимофей : Ура!
19:56:36	 От  Pavel M. : Прива всем!
19:59:00	 От  Пользователь Отус : https://goo.gl/forms/tRU3enLE1Gn7xgEP2
20:00:38	 От  Пользователь Отус : проверяю запись
20:01:52	 От  georkasapidi : +
20:01:53	 От  Дмитрий Лабазкин : отлично
20:01:55	 От  Хорев Тимофей : Отлично
20:01:55	 От  Александр Соловьев : хорошо
20:01:56	 От  Антон А : ок
20:01:57	 От  Sergey Lyakhov : отлично
20:01:59	 От  user : ОК
20:02:10	 От  Хорев Тимофей : Все так
20:02:12	 От  user : ок
20:02:14	 От  Александр Соловьев : +
20:02:15	 От  max : +
20:02:16	 От  Дмитрий Лабазкин : +
20:04:30	 От  Дмитрий Лабазкин : +
20:04:31	 От  Хорев Тимофей : Слышал
20:04:33	 От  user : слышал
20:04:39	 От  Сергей Антропянский : да
20:04:39	 От  Pavel M. : переводим все в цифорки
20:04:40	 От  max : проходили в универе
20:04:43	 От  Damir : нет, что такое машинное обучение
20:04:43	 От  Sergey Lyakhov : метод наименьших квадратов
20:04:53	 От  Александр Соловьев : гуглил
20:06:24	 От  Sergey Lyakhov : только у меня отвратительный звук?
20:06:36	 От  Damir : можно ли сказать, что это обратное от статистики
20:06:43	 От  georkasapidi : со звуков все ок
20:08:57	 От  Pavel M. : снижение размерности не относится и к обучению с учителем?
20:11:18	 От  Сергей Антропянский : классификация: сегментация клиентов
20:11:25	 От  Александр Соловьев : регрессия - прогноз баллов  у ползователя по кол-ву ответов и лаков
20:11:31	 От  Хорев Тимофей : Регрессия по текущим измерениям датчиков скорости и координатам предсказать координаты в следующий момент времени
20:11:32	 От  Дмитрий Лабазкин : регрессия - температура на основе погодных условий, классификация - кредитный скоринг
20:11:36	 От  rinatshigapov : классификация - email - спам/не спам
20:11:36	 От  Pavel M. : Регрессия -расчет стоимости на бу автомобиль
20:11:53	 От  Хорев Тимофей : Классификация, распознавание бракованного товара по весу и фотографии
20:11:56	 От  Pavel M. : Классификация - Болен человек каким-то заболиванием или нет
20:12:04	 От  Николай Горев : Задача регрессии: спрогнозировать температуру завтра, задача классификации: спрогнозировать, будет ли завтра дождь
20:12:43	 От  georkasapidi : регрессия - предсказать результат спортивного матча, классификация - к какому типу отностся клетка (раковая или нормальная)
20:15:29	 От  Дмитрий Лабазкин : зависать стало 
20:20:37	 От  Vitaly Zubov : 1
20:20:37	 От  georkasapidi : 1
20:20:38	 От  max : 1
20:20:39	 От  Александр Соловьев : 1
20:20:39	 От  Pavel M. : Тоесть гипотетически можно подобрать так что предсказание будет 100%(Если не берем шум)
20:20:40	 От  Damir : 1
20:20:40	 От  Дмитрий Лабазкин : 1
20:20:40	 От  Антон А : 1
20:20:40	 От  Andrey Y : 1
20:20:40	 От  Елена Ю. : 1
20:20:42	 От  Хорев Тимофей : 1
20:20:43	 От  Николай Горев : 1
20:20:44	 От  Сергей Антропянский : 1
20:20:45	 От  user : 1
20:20:46	 От  Oleg Erasov : 1
20:21:04	 От  Pavel M. : закон природы?
20:23:05	 От  max : 2
20:23:05	 От  Александр Соловьев : 2
20:23:07	 От  Антон А : 2
20:23:07	 От  Vitaly Zubov : 2
20:23:07	 От  georkasapidi : 2
20:23:07	 От  Oleg Erasov : 2
20:23:08	 От  Елена Ю. : 2
20:23:09	 От  Николай Горев : 2
20:23:09	 От  Andrey Y : 2
20:23:09	 От  user : 2
20:23:13	 От  Дмитрий Лабазкин : 2
20:23:42	 От  Хорев Тимофей : т.е. это оценка ошибки нашего предсказания
20:24:48	 От  Елена Ю. : шум портит нам эту "ошибку"
20:26:17	 От  Дмитрий Лабазкин : это нужно в собственном jupyter нужно?
20:26:19	 От  user : в чате писать?
20:26:20	 От  Дмитрий Лабазкин : реализовать
20:28:20	 От  Николай Горев : 121.3684788455632

20:28:38	 От  georkasapidi : 156.32077655454924
20:29:01	 От  georkasapidi : у меня еще izip не заимпортился
20:32:33	 От  georkasapidi : градиент напоминает
20:35:11	 От  max : тогда производная должна была быть по бетта с крышечкой
20:38:09	 От  user : почему 4
20:38:20	 От  Дмитрий Лабазкин : вы бета ноль подставили
20:41:54	 От  Дмитрий Лабазкин : 3
20:41:54	 От  max : 3
20:41:55	 От  user : 3
20:41:56	 От  Николай Горев : 3
20:41:56	 От  Хорев Тимофей : 3
20:41:57	 От  Andrey Y : 3
20:41:59	 От  Oleg Erasov : 3
20:42:03	 От  georkasapidi : 3 (конечная формула :))
20:42:51	 От  user : все понтно с 4 b0 подставили
20:45:22	 От  s : там унити уберут?
20:46:19	 От  georkasapidi : 3.922074824522173 3.151727962450419
20:48:15	 От  Николай Горев : 3.9849468127663696 3.0567741722248294
20:50:39	 От  Хорев Тимофей : 3.89 3.03 (Считал на ноуте, поэтому только так)
20:53:44	 От  georkasapidi : 4
20:53:47	 От  Andrey Y : 4
20:53:47	 От  max : 4
20:53:49	 От  Oleg Erasov : 4
20:53:49	 От  user : 4
20:53:49	 От  Дмитрий Лабазкин : 4
20:53:49	 От  Николай Горев : 4
20:53:51	 От  Сергей Антропянский : 4
20:57:15	 От  Хорев Тимофей : x^0
21:03:21	 От  Pavel M. : Получается множественная регрессия это объедененияе регрессий для каждого признака?
21:03:58	 От  rinatshigapov : обобщение на многомерный случай
21:06:39	 От  s : например, предсказание кровяного давления от возраста и веса
21:14:14	 От  georkasapidi : 6.09132287
21:14:34	 От  georkasapidi : кек) видимо что-то напутал
21:14:58	 От  georkasapidi : array([[4.03671049],
       [2.94491653],
       [1.02152035]])
21:14:59	 От  Damir : array([[ 4.33504526],       [ 2.89620221],       [ 0.92374589]])
21:15:07	 От  georkasapidi : во, я не перезапустил ноутбук
21:15:11	 От  Николай Горев : array([[4.02553794],
       [2.91440744],
       [1.00958918]])


21:15:26	 От  Andrey Y : array([[ 3.97834914],       [ 2.87812442],       [ 1.05461691]])
21:16:20	 От  Дмитрий Лабазкин : может не быть обратной матрицы?
21:21:01	 От  Дмитрий Лабазкин : 5
21:21:01	 От  user : 5
21:21:02	 От  Хорев Тимофей : 5
21:21:03	 От  max : 5
21:21:04	 От  Andrey Y : 5
21:21:05	 От  rinatshigapov : 5
21:21:06	 От  Николай Горев : 5
21:21:09	 От  Oleg Erasov : 5
21:26:25	 От  Хорев Тимофей : 6
21:26:27	 От  georkasapidi : 6
21:26:27	 От  Николай Горев : 6
21:26:28	 От  rinatshigapov : 6
21:26:28	 От  max : 6
21:26:29	 От  Andrey Y : 6
21:26:29	 От  user : 6
21:26:31	 От  Дмитрий Лабазкин : а мы получается фичи генерили без bias, то есть без intercept?
21:26:36	 От  Сергей Антропянский : 6
21:26:50	 От  georkasapidi : можно еще раз - bias для чего был?
21:27:11	 От  Сергей Антропянский : а более сложные функции: экспонента , логарифм ?
21:27:13	 От  Хорев Тимофей : А для не полиномиальных функций можно так делать? У нас например в примерах были данные расположенные по кругу.
21:27:20	 От  Антон А : по картинке не всегда понятно какой степени полином, как определить?
21:27:23	 От  Дмитрий Лабазкин : а у нас он False же bias
21:27:41	 От  Сергей Антропянский : линеаризация можно сделать от экспоненты
21:28:56	 От  Дмитрий Лабазкин : понятно, спасибо
21:29:55	 От  Сергей Антропянский : какой встроенной функцией считается R^2 ?
21:31:38	 От  Дмитрий Лабазкин : sklearn.metrics.r2_score
21:32:24	 От  Vitaly Zubov : это переобучение?
21:33:16	 От  Александр Соловьев : судя по кортинке надо взять 3 степень но только среднюю часть?
21:34:02	 От  user : 7
21:34:07	 От  Хорев Тимофей : 7
21:34:08	 От  georkasapidi : 7
21:34:09	 От  Елена Ю. : и как эту ошибку выявить 
21:34:09	 От  Дмитрий Лабазкин : 7
21:34:11	 От  max : 7
21:34:12	 От  Антон А : 7
21:34:13	 От  Andrey Y : 7
21:34:14	 От  Елена Ю. : если нарисовать нельзя
21:39:47	 От  Pavel M. : альфу тоже можно подбирать?
21:39:54	 От  Дмитрий Лабазкин : а добавление диагональной матрицы гарантирует независимость строк?
21:40:21	 От  Pavel M. : тоесть при прочих равных она раотает лучше линейной?
21:43:19	 От  Pavel M. : тоесть не будет переобучения?
21:44:52	 От  georkasapidi : сингулярнось - это зависимость признаков?
21:45:10	 От  Станислав : мультиколлинеарность
21:45:32	 От  Pavel M. : тоесть при приминение полинома можно схлопотать переобучение
21:46:17	 От  Пользователь Отус : https://goo.gl/forms/Vkg6hXbn89yh7vA03
21:46:50	 От  Пользователь Отус : Обратная связьhttps://otus.ru/polls/schedule/BigData
21:47:00	 От  Дмитрий Лабазкин : Про то что нужно всегда использовать регуляризацию - это про данные алгоритмы или вообще? или некоторые алгоритмы сами имеют встроенную регуляризацию
21:48:26	 От  max : L1 и L2 где-то уже реализованы?
21:48:49	 От  Дмитрий Лабазкин : скоро DataFest)
21:50:43	 От  Дмитрий Лабазкин : но если мы уменьшаем кол-во фичей
21:50:54	 От  Дмитрий Лабазкин : регуляризацией
21:50:56	 От  Дмитрий Лабазкин : лассо
21:51:55	 От  rinatshigapov : ускорить можно, используя стохастический градиентный спуск
21:53:01	 От  Дмитрий Лабазкин : спасибо!
21:53:04	 От  Хорев Тимофей : Спасибо!
21:53:10	 От  rinatshigapov : спасибо!
21:53:10	 От  georkasapidi : спасибо!
21:53:14	 От  user : спасибо!
21:53:15	 От  Александр Соловьев : спасибо
21:53:24	 От  Сергей Антропянский : спасибо
21:53:29	 От  Pavel M. : Реализация в склерне конечно радует 4 строчки и в путь
21:53:49	 От  Станислав : спасибо, было оочень полезно!!
21:53:53	 От  Andrey Y : Кстати, в Zoom-конференции можно звонить по телефону
21:54:06	 От  Pavel M. : а насколько часто юзают эластик
21:54:08	 От  Andrey Y : Если во звуком вообще ничего не получится
21:54:49	 От  georkasapidi : задание будет на следующем уроке?
21:55:12	 От  georkasapidi : круто
21:57:14	 От  Pavel M. : Получается что с помощь ласо и ридж можно пообрать лучшую моель7
21:58:45	 От  Pavel M. : Теоретически человек который не разбирается в теории лин регрессии может также получить этот результат?
21:59:38	 От  Pavel M. : Спасибо за ответы.
22:00:07	 От  Pavel M. : еще 1 вопрос
22:00:24	 От  Елена Ю. : уиииии)))))
22:00:25	 От  user : знатный зверюга)
22:00:27	 От  Станислав : 🐻
22:00:33	 От  Александр Соловьев : ))
22:01:17	 От  Pavel M. : Получается с помощью этих алгоритмов мы имеем высокую вероятность получить хорошее предсказание.Если разобратся с шумом и тд.
22:01:47	 От  Pavel M. : я бы даже сказал ы обреченны получить хороший рещултат
22:03:29	 От  Pavel M. : Спасибо у меня все)
22:03:52	 От  Елена Ю. : спасибо
22:04:12	 От  Pavel M. : до свидания.
